import os
import sys
import streamlit as st

# --- LangChain: ìµœì‹  ë¶„ë¦¬ íŒ¨í‚¤ì§€ ìž„í¬íŠ¸ ---
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import (
    GoogleGenerativeAIEmbeddings,
    ChatGoogleGenerativeAI,
)
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains import (
    create_history_aware_retriever,
    create_retrieval_chain,
)
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.chat_message_histories.streamlit import (
    StreamlitChatMessageHistory,
)
from langchain_core.runnables.history import RunnableWithMessageHistory

# --- Chroma (sqlite ëŒ€ì²´ìš© pysqlite3 íŒ¨ì¹˜: Streamlit Cloud í˜¸í™˜) ---
__import__("pysqlite3")
sys.modules["sqlite3"] = sys.modules.pop("pysqlite3")  # noqa: E402
from langchain_chroma import Chroma  # noqa: E402


# =========================
# í™˜ê²½ ì„¤ì • (Secrets/Env)
# =========================
# Streamlit Secrets ì— GOOGLE_API_KEY ê°€ ìžˆì–´ì•¼ í•©ë‹ˆë‹¤.
GOOGLE_API_KEY = st.secrets.get("GOOGLE_API_KEY", os.getenv("GOOGLE_API_KEY"))
if not GOOGLE_API_KEY:
    st.error("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Settings â†’ Secrets ì— ì¶”ê°€í•˜ì„¸ìš”.")
    st.stop()
os.environ["GOOGLE_API_KEY"] = GOOGLE_API_KEY  # langchain_google_genai ì—ì„œ ìžë™ ì°¸ì¡°


# =========================
# ìºì‹œ ìœ í‹¸
# =========================
@st.cache_resource
def load_pdf_pages(file_path: str):
    """PDF ë¡œë“œ (ë¶„í• ì€ ë³„ë„ ë‹¨ê³„ì—ì„œ ìˆ˜í–‰)."""
    loader = PyPDFLoader(file_path)
    pages = loader.load()  # load_and_split ëŒ€ì‹  ì•ˆì „í•˜ê²Œ loadë§Œ
    return pages


@st.cache_resource
def build_vectorstore(docs, persist_directory: str = "./chroma_db"):
    """ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë‚˜ëˆˆ ë’¤ Chroma ë²¡í„°DB êµ¬ì¶• (ìž„ë² ë”©: Gemini)."""
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=120)
    chunks = splitter.split_documents(docs)

    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001",
        google_api_key=GOOGLE_API_KEY,
    )
    vs = Chroma.from_documents(
        chunks,
        embedding=embeddings,
        persist_directory=persist_directory,
        collection_name="library-regulations",
    )
    return vs


@st.cache_resource
def get_or_create_vectorstore(docs, persist_directory: str = "./chroma_db"):
    """ê¸°ì¡´ Chroma DBê°€ ìžˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ìƒì„±."""
    if os.path.exists(persist_directory):
        embeddings = GoogleGenerativeAIEmbeddings(
            model="models/embedding-001",
            google_api_key=GOOGLE_API_KEY,
        )
        return Chroma(
            persist_directory=persist_directory,
            embedding_function=embeddings,
            collection_name="library-regulations",
        )
    return build_vectorstore(docs, persist_directory=persist_directory)


# =========================
# RAG ì²´ì¸ ì´ˆê¸°í™”
# =========================
@st.cache_resource
def initialize_rag_chain(selected_model: str):
    # PDF ê²½ë¡œ (ë°°í¬ í™˜ê²½ì— ë§žì¶”ì–´ ê²½ë¡œ í™•ì¸)
    file_path = r"/mount/src/library_chatbot1/[ì±—ë´‡í”„ë¡œê·¸ëž¨ë°ì‹¤ìŠµ] ë¶€ê²½ëŒ€í•™êµ ê·œì •ì§‘.pdf"
    pages = load_pdf_pages(file_path)
    vectorstore = get_or_create_vectorstore(pages)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

    # 1) ì´ì „ ëŒ€í™” ë§¥ë½ì„ ë°˜ì˜í•´ "ë…ë¦½ ì§ˆë¬¸"ìœ¼ë¡œ ìž¬êµ¬ì„±í•˜ëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    contextualize_q_system_prompt = (
        "Given a chat history and the latest user question, rewrite the question as a "
        "standalone one that can be understood without the chat history. "
        "Do NOT answer; only rewrite if needed."
    )
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    # 2) ì‹¤ì œ Q&A í”„ë¡¬í”„íŠ¸ (ê·¼ê±° ë¬¸ë§¥ë§Œ ì‚¬ìš©, ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€)
    qa_system_prompt = (
        "You are an assistant for question answering. Use ONLY the following retrieved "
        "context to answer. If you don't know, say you don't know. "
        "Answer in Korean, use honorifics, keep it precise, and include a friendly emoji.\n\n"
        "{context}"
    )
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    # 3) LLM (Gemini)
    llm = ChatGoogleGenerativeAI(
        google_api_key=GOOGLE_API_KEY,
        model=selected_model,  # e.g., "models/gemini-1.5-flash"
        temperature=0.3,
    )

    # 4) ì²´ì¸ êµ¬ì„±
    history_aware_retriever = create_history_aware_retriever(
        llm=llm, retriever=retriever, prompt=contextualize_q_prompt
    )
    question_answer_chain = create_stuff_documents_chain(llm=llm, prompt=qa_prompt)
    rag_chain = create_retrieval_chain(
        retriever=history_aware_retriever, combine_docs_chain=question_answer_chain
    )
    return rag_chain


# =========================
# UI
# =========================
st.set_page_config(page_title="êµ­ë¦½ë¶€ê²½ëŒ€ ë„ì„œê´€ ê·œì • Q&A", page_icon="ðŸ“š")
st.header("êµ­ë¦½ë¶€ê²½ëŒ€ ë„ì„œê´€ ê·œì • Q&A ì±—ë´‡ ðŸ’¬ðŸ“š")

# Gemini ëª¨ë¸ëª…ì€ langchain_google_genai ê¸°ì¤€ìœ¼ë¡œ "models/..." í˜•ì‹ ê¶Œìž¥
model_label = st.selectbox(
    "Gemini ëª¨ë¸ ì„ íƒ",
    ("gemini-1.5-flash", "gemini-1.5-pro"),
    index=0,
)
MODEL_NAME = (
    f"models/{model_label}" if not model_label.startswith("models/") else model_label
)

rag_chain = initialize_rag_chain(MODEL_NAME)

# ëŒ€í™” ê¸°ë¡ (Streamlit ì „ìš© ë©”ëª¨ë¦¬)
chat_history = StreamlitChatMessageHistory(key="chat_messages")

# ì²´ì¸ì— ëŒ€í™” ê¸°ë¡ ì—°ê²°
conversational_rag_chain = RunnableWithMessageHistory(
    rag_chain,
    lambda session_id: chat_history,  # history ê°ì²´ ê³µê¸‰
    input_messages_key="input",
    history_messages_key="history",
    output_messages_key="answer",
)

# ì´ˆê¸° ì•ˆë‚´ ë©”ì‹œì§€ (ìµœì´ˆ 1íšŒ)
if "messages_initialized" not in st.session_state:
    st.session_state["messages_initialized"] = True
    st.chat_message("ai").write(
        "êµ­ë¦½ë¶€ê²½ëŒ€ ë„ì„œê´€ ê·œì •ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”! í•„ìš”í•œ ì¡°í•­ì„ ì°¾ì•„ë“œë¦´ê²Œìš”. ðŸ™‚"
    )

# ê¸°ì¡´ ë©”ì‹œì§€ ë Œë”
for msg in chat_history.messages:
    st.chat_message(msg.type).write(msg.content)

# ìž…ë ¥ì°½
if user_q := st.chat_input("ì§ˆë¬¸ì„ ìž…ë ¥í•˜ì„¸ìš”"):
    st.chat_message("human").write(user_q)
    with st.chat_message("ai"):
        with st.spinner("ê²€í†  ì¤‘..."):
            # ì„¸ì…˜ IDëŠ” ë™ì¼ ì‚¬ìš©ìž ìŠ¤ë ˆë“œ ì‹ë³„ìš©
            config = {"configurable": {"session_id": "pknu-library"}}
            result = conversational_rag_chain.invoke({"input": user_q}, config=config)

            answer = result.get("answer", "")
            st.write(answer)

            # ì°¸ê³  ë¬¸ì„œ(ì»¨í…ìŠ¤íŠ¸) ê³µê°œ
            if "context" in result:
                with st.expander("ðŸ”Ž ì°¸ê³  ë¬¸ì„œ ë³´ê¸°"):
                    for i, doc in enumerate(result["context"], start=1):
                        src = doc.metadata.get("source", "source not found")
                        st.markdown(f"**[{i}]** {src}")
                        st.caption(doc.page_content[:500] + ("..." if len(doc.page_content) > 500 else ""))
